{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('reviews.txt', 'r')\n",
    "reviews = list(map(lambda x:x[:-1], file.readlines()))\n",
    "file.close()\n",
    "\n",
    "file = open('labels.txt', 'r')\n",
    "labels = list(map(lambda x:x[:-1], file.readlines()))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'review': reviews, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].value_counts().plot(kind='barh', figsize=(15,2));\n",
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_count = Counter()\n",
    "negative_words_count = Counter()\n",
    "total_words_count = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, review in enumerate(df['review']):\n",
    "    if df['label'][num-1] == 'POSITIVE':\n",
    "        for word in review.split(\" \"):\n",
    "            positive_words_count[word] += 1\n",
    "            total_words_count[word] += 1\n",
    "    else:\n",
    "        for word in review.split(\" \"):\n",
    "            negative_words_count[word] += 1\n",
    "            total_words_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words_count[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Positive Negative Ratio for the most Repeating Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Positive Negative Ratio for the most Repeating Words\n",
    "positive_negative_ratios = Counter()\n",
    "margin = 100\n",
    "\n",
    "for word, count in list(total_words_count.most_common()):\n",
    "    if count > margin:\n",
    "        ratio = positive_words_count[word] / negative_words_count[word]+1 # Avoiding Zero Division\n",
    "        positive_negative_ratios[word] = ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Log of the Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, ratio in positive_negative_ratios:\n",
    "    if ratio > 1:\n",
    "        positive_negative_ratios[word] = np.log(ratio)    \n",
    "    else:\n",
    "        positive_negative_ratios[word] = -np.log(1/(ratio+0.001)) # Avoiding Zero Division    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Positive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Positive Words\n",
    "positive_negative_ratios.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Negative Words\n",
    "reversed(positive_negative_ratios.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Text into Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = total_words_count.keys()\n",
    "size = len(words)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_index = {}\n",
    "\n",
    "for index, word in enumerate(words):\n",
    "    words_index[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = np.zeros((1,size))\n",
    "input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input(review):\n",
    "    global input_layer\n",
    "    \n",
    "    # Clear previuos inputs\n",
    "    input_layer *= 0\n",
    "    \n",
    "    # Getting the number of words in the review\n",
    "    for word in review.split(\" \"):\n",
    "        input_layer[0][words_index[word]] += 1\n",
    "    return input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_input(df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Sentiment Analysis Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=10, learning_rate=0.01):\n",
    "        self.pre_process(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "    def pre_process(self, reviews, labels):\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        \n",
    "        label_vocab =set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(word)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.review_vocab_size = len(review_vocab)\n",
    "        self.label_vocab_size = len(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.words_index = {}\n",
    "        for index, word in enumerate(self.review_vocab):\n",
    "            words_index[word] = index\n",
    "            \n",
    "        self.labels_index = {}\n",
    "        for index, label in enumerate(self.label_vocab):\n",
    "            labels_index[label] = index\n",
    "            \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))\n",
    "        self.learnig_rate = learnig_rate\n",
    "        \n",
    "        self.input_layer = np.zeros((1, input_nodes))\n",
    "        \n",
    "    def update_input(self, review):\n",
    "        # Clear previuos inputs\n",
    "        self.input_layer *= 0\n",
    "\n",
    "        # Getting the number of words in the review\n",
    "        for word in review.split(\" \"):\n",
    "            self.input_layer[0][words_index[word]] += 1\n",
    "        return self.input_layer\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_dervitive(self, ouput):\n",
    "        return ouput * (1-output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        assert(len(training_reviews)==len(training_labels))\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(trainging_reviews):\n",
    "            review = review\n",
    "            label = training_labels[num]\n",
    "            \n",
    "            self.update_input_layer(review)\n",
    "            layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            # Backward Pass\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_dervitive(layer_2)\n",
    "            \n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            self.weights_1_2 = layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            self.weights_0_1 = self.input_layer.T.dot(layer_1_delta) * self.learning_rate\n",
    "            \n",
    "            if np.abs(layer_2_error) <0.5:\n",
    "                correct +=1\n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        correct = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(testing_reviews):\n",
    "            pred = self.run(review)\n",
    "            \n",
    "            if pred == testing_label[num]:\n",
    "                correct += 1\n",
    "                \n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "    def run(self, review):\n",
    "        self.update_input_layer(review.lower())\n",
    "        layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "        if np.abs(layer_2_error) <0.5:\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.test(reviews[-1000:], labels[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000], labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000], labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000], labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Noise\n",
    "white spaces and so on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in reviews[0].spllit(\" \"):\n",
    "    review_counter[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing Noise & Editing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=10, learning_rate=0.01):\n",
    "        self.pre_process(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "    def pre_process(self, reviews, labels):\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        \n",
    "        label_vocab =set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(word)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.review_vocab_size = len(review_vocab)\n",
    "        self.label_vocab_size = len(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.words_index = {}\n",
    "        for index, word in enumerate(self.review_vocab):\n",
    "            words_index[word] = index\n",
    "            \n",
    "        self.labels_index = {}\n",
    "        for index, label in enumerate(self.label_vocab):\n",
    "            labels_index[label] = index\n",
    "            \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))\n",
    "        self.learnig_rate = learnig_rate\n",
    "        \n",
    "        self.input_layer = np.zeros((1, input_nodes))\n",
    "    \n",
    "    #### Here the change ####\n",
    "    #### Instead of puttinh the counts in, we add 1 if the word exists,\n",
    "    #### thats to avoid the inbalance according to the number of spaces, or\n",
    "    #### the number of  unuseful characters, but noise stell exist in the input data.\n",
    "    #### why dont we remove noise from data using NLTK or any other library?\n",
    "    def update_input(self, review):\n",
    "        # Clear previuos inputs\n",
    "        self.input_layer *= 0\n",
    "\n",
    "        # Getting the number of words in the review\n",
    "        for word in review.split(\" \"):\n",
    "            #### Exactly here\n",
    "            self.input_layer[0][words_index[word]] = 1 #=1 instead of +=1\n",
    "        return self.input_layer\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_dervitive(self, ouput):\n",
    "        return ouput * (1-output)\n",
    "    \n",
    "    def train(self, training_reviews, training_labels):\n",
    "        assert(len(training_reviews)==len(training_labels))\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(trainging_reviews):\n",
    "            review = review\n",
    "            label = training_labels[num]\n",
    "            \n",
    "            self.update_input_layer(review)\n",
    "            layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            # Backward Pass\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_dervitive(layer_2)\n",
    "            \n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            self.weights_1_2 = layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            self.weights_0_1 = self.input_layer.T.dot(layer_1_delta) * self.learning_rate\n",
    "            \n",
    "            if np.abs(layer_2_error) <0.5:\n",
    "                correct +=1\n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        correct = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(testing_reviews):\n",
    "            pred = self.run(review)\n",
    "            \n",
    "            if pred == testing_label[num]:\n",
    "                correct += 1\n",
    "                \n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "    def run(self, review):\n",
    "        self.update_input_layer(review.lower())\n",
    "        layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "        if np.abs(layer_2_error) <0.5:\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000], labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.test(reviews[-1000:], labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Ineffieciencies in the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=10, learning_rate=0.01):\n",
    "        self.pre_process(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "    def pre_process(self, reviews, labels):\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        \n",
    "        label_vocab =set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(word)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.review_vocab_size = len(review_vocab)\n",
    "        self.label_vocab_size = len(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.words_index = {}\n",
    "        for index, word in enumerate(self.review_vocab):\n",
    "            words_index[word] = index\n",
    "            \n",
    "        self.labels_index = {}\n",
    "        for index, label in enumerate(self.label_vocab):\n",
    "            labels_index[label] = index\n",
    "            \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))\n",
    "        self.learnig_rate = learnig_rate\n",
    "        \n",
    "        self.input_layer = np.zeros((1, input_nodes))\n",
    "    \n",
    "\n",
    "    def update_input(self, review):\n",
    "        # Clear previuos inputs\n",
    "        self.input_layer *= 0\n",
    "\n",
    "        # Getting the number of words in the review\n",
    "        for word in review.split(\" \"):\n",
    "            #### Exactly here\n",
    "            self.input_layer[0][words_index[word]] = 1 #=1 instead of +=1\n",
    "        return self.input_layer\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_dervitive(self, ouput):\n",
    "        return ouput * (1-output)\n",
    "    \n",
    "    #### Here the change ####\n",
    "    #### Instead of calculating everything, the zero inputs and the one inputs,\n",
    "    #### we will calculate the one inputs only and save the time of the zero inputs,\n",
    "    #### by taking the indices of the one inputs and map them into the weights inputs directly,\n",
    "    #### that saves the time of multiplying the zeros(words not exist) by the wieghts that will\n",
    "    #### become zero in the end, its more than 10 times faster.\n",
    "    \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        \n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if word in self.words_index.keys():\n",
    "                    indices.add(self.words_index[word])\n",
    "                training_reviews.append(list(indices))\n",
    "        \n",
    "        \n",
    "        assert(len(training_reviews)==len(training_labels))\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(trainging_reviews):\n",
    "            review = review\n",
    "            label = training_labels[num]\n",
    "            \n",
    "            # self.update_input_layer(review)\n",
    "            #layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            # Backward Pass\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_dervitive(layer_2)\n",
    "            \n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            #self.weights_1_2 = layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            #self.weights_0_1 = self.input_layer.T.dot(layer_1_delta) * self.learning_rate\n",
    "            \n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            \n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= self.layer_1_delta[0] * self.learning_rate\n",
    "            \n",
    "            if np.abs(layer_2_error) <0.5:\n",
    "                correct +=1\n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        correct = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(testing_reviews):\n",
    "            pred = self.run(review)\n",
    "            \n",
    "            if pred == testing_label[num]:\n",
    "                correct += 1\n",
    "                \n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "    def run(self, review):\n",
    "        self.update_input_layer(review.lower())\n",
    "        layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "        if np.abs(layer_2_error) <0.5:\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000] * 2, labels[:-1000] * 2) # Multiple iterations training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.test(reviews[-1000:], labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Noise Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataStructure, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = np.histogram(list(map(lambda x:x[1], positive_negative_ratios.most_common())), \n",
    "                          density=True, bins=100,normed=True)\n",
    "\n",
    "p = figure(tools='pan,wheel_zoom,reset,save', \n",
    "           toolbar_location='above',\n",
    "           title='Word Positive/Negative Affinity Distribution')\n",
    "\n",
    "p.quad(top=hist, bootom=0, left=edges[:-1], right=edges[1:], line_color='#555555')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = Counter()\n",
    "\n",
    "for word, cnt in total_words_count.most_commn():\n",
    "    frequency[cnt] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, edges = np.histogram(list(map(lambda x:x[1], frequency.most_common())), \n",
    "                          density=True, bins=100,normed=True)\n",
    "\n",
    "p = figure(tools='pan,wheel_zoom,reset,save', \n",
    "           toolbar_location='above',\n",
    "           title='The Frequency Distribution of hte Words in our Corpus')\n",
    "\n",
    "p.quad(top=hist, bootom=0, left=edges[:-1], right=edges[1:], line_color='#555555')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=10, learning_rate=0.01):\n",
    "        self.pre_process(reviews, labels)\n",
    "        self.init_network(len(self.review_vocab), hidden_nodes, 1, learning_rate)\n",
    "    \n",
    "    #### Here the change ####\n",
    "    \n",
    "    def pre_process(self, reviews, labels, min_count, polarity_cutoff):\n",
    "        \n",
    "        positive_words_count = Counter()\n",
    "        negative_words_count = Counter()\n",
    "        total_words_count = Counter()\n",
    "        \n",
    "        for i in range(len(reviews)):\n",
    "            if labels[i] == 'POSITIVE':\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_words_count[word] += 1\n",
    "                    total_words_count[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_words_count[word] += 1\n",
    "                    total_words_count[word] += 1\n",
    "            \n",
    "        positive_negative_ratios = Counter()\n",
    "        \n",
    "        for term, count in list(total_words_count.most_common()):\n",
    "            if count >=50:\n",
    "                ratio = positive_words_count[term] / negative_words_count[word]+1 # Avoiding Zero Division\n",
    "                positive_negative_ratios[word] = ratio\n",
    "        \n",
    "        \n",
    "        for word, ratio in positive_negative_ratios.most_common():\n",
    "            if ratio > 1:\n",
    "                positive_negative_ratios[word] = np.log(ratio)    \n",
    "            else:\n",
    "                positive_negative_ratios[word] = -np.log(1/(ratio+0.001)) # Avoiding Zero Division    \n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if total_words_count[word] > min_count:\n",
    "                    if word in positive_negative_ratios.keys():\n",
    "                        if (positive_negative_ratios[word] >= polarity_cutoff) or (positive_negative_ratios[word] <= -polarity_cutoff):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "                        \n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab =set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(word)\n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.review_vocab_size = len(review_vocab)\n",
    "        self.label_vocab_size = len(label_vocab)\n",
    "        \n",
    "        \n",
    "        self.words_index = {}\n",
    "        for index, word in enumerate(self.review_vocab):\n",
    "            words_index[word] = index\n",
    "            \n",
    "        self.labels_index = {}\n",
    "        for index, label in enumerate(self.label_vocab):\n",
    "            labels_index[label] = index\n",
    "            \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))\n",
    "        self.learnig_rate = learnig_rate\n",
    "        \n",
    "        self.input_layer = np.zeros((1, input_nodes))\n",
    "    \n",
    "\n",
    "    def update_input(self, review):\n",
    "        # Clear previuos inputs\n",
    "        self.input_layer *= 0\n",
    "\n",
    "        # Getting the number of words in the review\n",
    "        for word in review.split(\" \"):\n",
    "            #### Exactly here\n",
    "            self.input_layer[0][words_index[word]] = 1 #=1 instead of +=1\n",
    "        return self.input_layer\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def sigmoid_output_2_dervitive(self, ouput):\n",
    "        return ouput * (1-output)\n",
    "    \n",
    "    \n",
    "    #### Instead of calculating everything, the zero inputs and the one inputs,\n",
    "    #### we will calculate the one inputs only and save the time of the zero inputs,\n",
    "    #### by taking the indices of the one inputs and map them into the weights inputs directly,\n",
    "    #### that saves the time of multiplying the zeros(words not exist) by the wieghts that will\n",
    "    #### become zero in the end, its more than 10 times faster.\n",
    "    \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        \n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if word in self.words_index.keys():\n",
    "                    indices.add(self.words_index[word])\n",
    "                training_reviews.append(list(indices))\n",
    "        \n",
    "        \n",
    "        assert(len(training_reviews)==len(training_labels))\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(trainging_reviews):\n",
    "            review = review\n",
    "            label = training_labels[num]\n",
    "            \n",
    "            # self.update_input_layer(review)\n",
    "            #layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            # Backward Pass\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_dervitive(layer_2)\n",
    "            \n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            #self.weights_1_2 = layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            #self.weights_0_1 = self.input_layer.T.dot(layer_1_delta) * self.learning_rate\n",
    "            \n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            \n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= self.layer_1_delta[0] * self.learning_rate\n",
    "            \n",
    "            if np.abs(layer_2_error) <0.5:\n",
    "                correct +=1\n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "        \n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        correct = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for num, review in enumerate(testing_reviews):\n",
    "            pred = self.run(review)\n",
    "            \n",
    "            if pred == testing_label[num]:\n",
    "                correct += 1\n",
    "                \n",
    "            reviews_per_second = num / (time.time() - start_time)\n",
    "            \n",
    "            progress = (100 * num/len(training_reviews))[:4]\n",
    "            \n",
    "            sys.stdout.write('\\rProgress: {}', 'Speed(review/sec): {}'.format(progress,reviews_per_second))\n",
    "    def run(self, review):\n",
    "        self.update_input_layer(review.lower())\n",
    "        layer_1 = self.input_layer.dot(self.weights_0_1)\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "        if np.abs(layer_2_error) <0.5:\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], min_count=20, polarity_cutoff=0.05, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000] * 2, labels[:-1000] * 2) # Multiple iterations training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.test(reviews[-1000:], labels[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentNetwork(reviews[:-1000], labels[:-1000], min_count=20, polarity_cutoff=0.8, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(reviews[:-1000] * 2, labels[:-1000] * 2) # Multiple iterations training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.test(reviews[-1000:], labels[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(focus='excellent'):\n",
    "    most_similar = Counter()\n",
    "    \n",
    "    for word in mlp_full.word_index.key():\n",
    "        most_similar[word] = np.dot(mlp_full.weights_0_1[mlp_fullword_index[word]], mlp_full.weights_0_1[mlp_full.word_index[word]])\n",
    "        \n",
    "        return most_similar.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_words('excellent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_words('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import colors\n",
    "\n",
    "words_to_visualize = list()\n",
    "\n",
    "for word, ratio in positive_negative_ratios.most_common():\n",
    "    if word in mlp_full.word_index.keys():\n",
    "        words_to_visualize.append(word)\n",
    "        \n",
    "for word, ratio in list(reversed(positive_negative_ratios.most_common())):\n",
    "    if word in mlp_full.word_index.keys():\n",
    "        words_to_visualize.append(word)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "colors_list = list()\n",
    "vectors_list = list()\n",
    "\n",
    "for word in words_to_visualize:\n",
    "    if word in positive_negative_ratios.keys():\n",
    "        vectors_list.append(mlp.full.weights_0_1[mlp_full.word_index[word]])\n",
    "        if positive_negative_ratios[word] > 0:\n",
    "            pos += 1\n",
    "            colors_list.append('#00ff00')\n",
    "        else:\n",
    "            neg += 1\n",
    "            colors_list.append('#000000')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tesne = TSNE(n_components=2, random_state=0)\n",
    "words_top_ted_tsne = tsne.fit_transform(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(tools='pan,wheel_zoom,reset,save', \n",
    "           toolbar_location='above',\n",
    "           title='Vector TSNE for most Polarized Words')\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0],\n",
    "                                    x2=words_top_ted_tsne[:,1],\n",
    "                                   names=words_to_visualize))\n",
    "\n",
    "p.scatter(x=x1, y=x2, size=8, source=source, color=colors_list)\n",
    "          \n",
    "word_labels = LabelSet(x=x1, y=x2, text='names', y_offset=6,\n",
    "          text_font_size='8pt', text_color='#555555',\n",
    "         source=source, text_align='center')\n",
    "\n",
    "p.add_layout(word_labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
